{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "from codebase.utils import MongoConnector\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = MongoConnector(\"../config.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1, Retrieve hashtags from Mongo\n",
    "\n",
    "Take following times from Mongo:\n",
    "> CPU times: user 9min 9s, sys: 11.7 s, total: 9min 20s  \n",
    "> Wall time: 13min 26s\n",
    "\n",
    "I have save them as pickle. Just read back them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# Weeks = [\"FirstWeek_March\", \"SecondWeek_March\", \"ThirdWeek_March\", \"FourthWeek_March\",\\\n",
    "#          \"FirstWeek_April\", \"SecondWeek_April\", \"ThirdWeek_April\", \"FourthWeek_April\"]\n",
    "\n",
    "# query = {}\n",
    "# query[\"hashtags\"] = {\n",
    "#     u\"$exists\": True\n",
    "# }\n",
    "\n",
    "# rows_list = []\n",
    "# for each_week in Weeks:\n",
    "#     conn.get_collection_cursor(each_week)\n",
    "#     docs = conn.data_streaming_from_collection(query=query)\n",
    "#     for i, doc in enumerate(docs):\n",
    "#         if i % 100000 == 0:\n",
    "#             print(f\"{each_week}: {i}\")\n",
    "        \n",
    "#         row_dict = dict()\n",
    "#         created_at = doc[\"created_at\"]\n",
    "#         for tag in doc[\"hashtags\"]:\n",
    "#             row_dict.update(created_at=created_at, hashtag=tag)\n",
    "#             rows_list.append(row_dict)\n",
    "\n",
    "# tagsDF = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle\n",
    "# tagsDF.to_pickle(path=\"./export/Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "#                  compression=\"gzip\")\n",
    "\n",
    "# read pickle\n",
    "tagsDF = pd.read_pickle(\"../export/Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "                  compression=\"gzip\")\n",
    "tagsDF.reset_index()\n",
    "tagsDF = tagsDF.set_index(\"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsDF[\"hashtag\"] = tagsDF[\"hashtag\"].apply(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Fuzzy Mapping (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagSet = set(tagsDF.hashtag.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tagDict = {}\n",
    "tagCandidates = set()\n",
    "editing_threshold = 90\n",
    "\n",
    "pbar = tqdm(total=len(tagSet))\n",
    "for tag in tagSet:\n",
    "    pbar.update(1)\n",
    "    # first case\n",
    "    if len(tagCandidates) == 0:\n",
    "        tagDict[tag] = tag\n",
    "        tagCandidates.add(tag)\n",
    "        continue \n",
    "    \n",
    "    most_sim_tag, sim = process.extractOne(tag, tagCandidates, scorer=fuzz.ratio)\n",
    "#     print(f\"{tag}: {most_sim_tag} {sim}\")\n",
    "    if sim >= editing_threshold:\n",
    "        tagDict[tag] = most_sim_tag\n",
    "    else:\n",
    "        tagDict[tag] = tag\n",
    "        tagCandidates.add(tag)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2, Resample counts of hashtag by week (or any time granuality)\n",
    "\n",
    "> CPU times: user 21min 44s, sys: 3.19 s, total: 21min 47s  \n",
    "> Wall time: 21min 46s\n",
    "\n",
    "> CPU times: user 18min 16s, sys: 5.52 s, total: 18min 22s  \n",
    "> Wall time: 18min 21s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tagsDF = tagsDF.loc['2020-03-01':'2020-04-26']\n",
    "grouped_tagsDF = tagsDF.groupby('hashtag').resample('W')['hashtag'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_tagsDF.to_pickle(\"./export/Counts-Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "#                   compression=\"gzip\")\n",
    "# grouped_tagsDF = pd.read_pickle(\"./export/Counts-Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "#                    compression=\"gzip\")\n",
    "# grouped_tagsDF.to_pickle(\"./export/Counts-lower-Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "#                   compression=\"gzip\")\n",
    "grouped_tagsDF = pd.read_pickle(\"../export/Counts-lower-Hashtags-of-Mar-Apr.pkl.gz\",\n",
    "                   compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = grouped_tagsDF.unstack('hashtag', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[[\"suicideprevention\", \"zerosuicide\", \"everylifematters\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3, Calculate count difference by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeekDiff = gdf.diff(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeekDiff.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4, Pick those hashtag with large count difference (Most Surging/Most Diminishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to [::-1] to get descending results\n",
    "# and [::1] to get ascending results\n",
    "sort_ids = WeekDiff.iloc[5].argsort()[::-1][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeekDiff.iloc[5][sort_ids]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HealthV",
   "language": "python",
   "name": "healthv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
